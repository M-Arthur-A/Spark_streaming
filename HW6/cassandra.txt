    ~  ssh student910_14@37.139.32.56 -p 22 -i /home/arthur/Project/GeekBrains/Spark_streaming/_ADDS/id_rsa_student910_14
Last login: Sun Sep 26 12:44:43 2021 from 109-252-29-224.nat.spd-mgts.ru
[student910_14@bigdataanalytics2-head-shdpt-v31-1-0 ~]$ ssh 10.0.0.19
Last login: Sun Sep 26 12:44:45 2021 from bigdataanalytics2-head-shdpt-v31-1-0.novalocal
[student910_14@bigdataanalytics2-worker-shdpt-v31-1-0 ~]$ /cassandra/bin/nodetool status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Tokens       Owns (effective)  Host ID                               Rack
UN  127.0.0.1  330.62 MiB  256          100.0%            41cf1164-fb96-4b14-b9c1-0f6c4c613a1b  rack1

[student910_14@bigdataanalytics2-worker-shdpt-v31-1-0 ~]$ /cassandra/bin/cqlsh
Connected to Test Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.11.8 | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.
cqlsh> CREATE KEYSPACE streaming_91014 WITH replication {'class': 'SimpleStrategy', 'replication_factor': 1};
0f8db57cbbfe8d67b257893', 'Jagvinder Dulay');
INSERT INTO streaming_91014.customer_names(cid, full_nameSyntaxException: line 1:49 no viable alternative at input '{' (CREATE KEYSPACE streaming_91014 WITH [replication] {...)
.......................................
.......................................
.......................................
[student910_14@bigdataanalytics2-worker-shdpt-v31-1-0 ~]$ /spark2.4/bin/pyspark --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5,com.datastax.spark:spark-cassandra-connector_2.11:2.4.2 --driver-memory 512m --driver-cores 1 --master local[2] --conf spark.sql.shuffle.partitions=20
Python 2.7.5 (default, Nov 16 2020, 22:23:17) 
[GCC 4.8.5 20150623 (Red Hat 4.8.5-44)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Warning: Ignoring non-Spark config property: hive.metastore.uris
Ivy Default Cache set to: /home/student910_14/.ivy2/cache
The jars for the packages stored in: /home/student910_14/.ivy2/jars
:: loading settings :: url = jar:file:/spark2.4/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
org.apache.spark#spark-sql-kafka-0-10_2.11 added as a dependency
com.datastax.spark#spark-cassandra-connector_2.11 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-f1187645-d267-4797-97b2-ce6c94146b1d;1.0
        confs: [default]
        found org.apache.spark#spark-sql-kafka-0-10_2.11;2.4.5 in central
        found org.apache.kafka#kafka-clients;2.0.0 in central
        found org.lz4#lz4-java;1.4.0 in central
        found org.xerial.snappy#snappy-java;1.1.7.3 in central
        found org.slf4j#slf4j-api;1.7.16 in central
        found org.spark-project.spark#unused;1.0.0 in central
        found com.datastax.spark#spark-cassandra-connector_2.11;2.4.2 in central
        found commons-beanutils#commons-beanutils;1.9.3 in central
        found commons-collections#commons-collections;3.2.2 in central
        found com.twitter#jsr166e;1.1.0 in central
        found org.joda#joda-convert;1.2 in central
        found io.netty#netty-all;4.0.33.Final in central
        found joda-time#joda-time;2.3 in central
        found org.scala-lang#scala-reflect;2.11.7 in central
downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.11/2.4.5/spark-sql-kafka-0-10_2.11-2.4.5.jar ...
        [SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.11;2.4.5!spark-sql-kafka-0-10_2.11.jar (125ms)
downloading https://repo1.maven.org/maven2/com/datastax/spark/spark-cassandra-connector_2.11/2.4.2/spark-cassandra-connector_2.11-2.4.2.jar ...
        [SUCCESSFUL ] com.datastax.spark#spark-cassandra-connector_2.11;2.4.2!spark-cassandra-connector_2.11.jar (304ms)
downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.7.3/snappy-java-1.1.7.3.jar ...
        [SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.7.3!snappy-java.jar(bundle) (98ms)
downloading https://repo1.maven.org/maven2/commons-beanutils/commons-beanutils/1.9.3/commons-beanutils-1.9.3.jar ...
        [SUCCESSFUL ] commons-beanutils#commons-beanutils;1.9.3!commons-beanutils.jar (49ms)
downloading https://repo1.maven.org/maven2/com/twitter/jsr166e/1.1.0/jsr166e-1.1.0.jar ...
        [SUCCESSFUL ] com.twitter#jsr166e;1.1.0!jsr166e.jar (46ms)
downloading https://repo1.maven.org/maven2/org/joda/joda-convert/1.2/joda-convert-1.2.jar ...
        [SUCCESSFUL ] org.joda#joda-convert;1.2!joda-convert.jar (41ms)
downloading https://repo1.maven.org/maven2/io/netty/netty-all/4.0.33.Final/netty-all-4.0.33.Final.jar ...
        [SUCCESSFUL ] io.netty#netty-all;4.0.33.Final!netty-all.jar (95ms)
downloading https://repo1.maven.org/maven2/joda-time/joda-time/2.3/joda-time-2.3.jar ...
        [SUCCESSFUL ] joda-time#joda-time;2.3!joda-time.jar (56ms)
downloading https://repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.7/scala-reflect-2.11.7.jar ...
        [SUCCESSFUL ] org.scala-lang#scala-reflect;2.11.7!scala-reflect.jar (180ms)
downloading https://repo1.maven.org/maven2/commons-collections/commons-collections/3.2.2/commons-collections-3.2.2.jar ...
        [SUCCESSFUL ] commons-collections#commons-collections;3.2.2!commons-collections.jar (61ms)
:: resolution report :: resolve 6419ms :: artifacts dl 1070ms
        :: modules in use:
        com.datastax.spark#spark-cassandra-connector_2.11;2.4.2 from central in [default]
        com.twitter#jsr166e;1.1.0 from central in [default]
>>> from pyspark.sql import SparkSession
>>> from pyspark.sql import functions as F
>>> from pyspark.sql.types import StringType, StructType
>>> 
>>> spark = SparkSession.builder.appName("cassandra_spark_app").getOrCreate()
>>> 
>>> spark.conf.set("spark.cassandra.connection.host", "localhost")
>>> checkpoint_location = "tmp/orders_checkpoint"
>>> 
>>> keyspace = "streaming_1004"
>>> 
>>> 
>>> # Console output
... def console_output(df, freq):
...     from datetime import datetime as dt
...     date = dt.now().strftime("%Y%m%d%H%M%S")
...     return df.writeStream \
...         .format("console") \
...         .trigger(processingTime='%s seconds' % freq) \
...         .option("checkpointLocation", checkpoint_location + "/" + date) \
...         .options(truncate=False) \
...         .start()
... 
>>> 
>>> # orders
... kafka_brokers = "bigdataanalytics2-worker-shdpt-v31-1-0:6667"
>>> kafka_topic = "st910_14_orders_topic_json"
>>> 
>>> raw_orders = spark.readStream. \
...     format("kafka"). \
...     option("kafka.bootstrap.servers", kafka_brokers). \
...     option("subscribe", kafka_topic). \
...     option("maxOffsetsPerTrigger", "20"). \
...     option("startingOffsets", "earliest"). \
...     load()

>>> 
>>> schema = StructType() \
...     .add("order_id", StringType()) \
...     .add("customer_id", StringType()) \
...     .add("order_status", StringType()) \
...     .add("order_purchase_timestamp", StringType()) \
...     .add("order_approved_at", StringType()) \
...     .add("order_delivered_carrier_date", StringType()) \
...     .add("order_delivered_customer_date", StringType()) \
...     .add("order_estimated_delivery_date", StringType())
>>> 
>>> 
>>> parsed_orders = raw_orders \
...     .select(F.from_json(F.col("value").cast("String"), schema).alias("value"), "offset") \
...     .select("value.*")
>>> stream = console_output(parsed_orders, 10)
21/10/02 13:04:26 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
-------------------------------------------                                     
Batch: 0
-------------------------------------------
+--------------------------------+--------------------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+
|order_id                        |customer_id                     |order_status|order_purchase_timestamp|order_approved_at  |order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|
+--------------------------------+--------------------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+
|e481f51cbdc54678b7cc49136f2d6af7|9ef432eb6251297304e76186b10a928d|delivered   |2017-10-02 10:56:33     |2017-10-02 11:07:15|2017-10-04 19:55:00         |2017-10-10 21:25:13          |2017-10-18 00:00:00          |
|53cdb2fc8bc7dce0b6741e2150273451|b0830fb4747a6c6d20dea0b8c802d7ef|delivered   |2018-07-24 20:41:37     |2018-07-26 03:24:27|2018-07-26 14:31:00         |2018-08-07 15:27:45          |2018-08-13 00:00:00          |
|47770eb9100c2d0c44946d9cf07ec65d|41ce2a54c0b03bf3443c3d931a367089|delivered   |2018-08-08 08:38:49     |2018-08-08 08:55:23|2018-08-08 13:50:00         |2018-08-17 18:06:29          |2018-09-04 00:00:00          |
|949d5b44dbf5de918fe9c16f97b45f8a|f88197465ea7920adcdbec7375364d82|delivered   |2017-11-18 19:28:06     |2017-11-18 19:45:59|2017-11-22 13:39:59         |2017-12-02 00:28:42          |2017-12-15 00:00:00          |
|ad21c59c0840e6cb83a9ceb5573f8159|8ab97904e6daea8866dbdbc4fb7aad2c|delivered   |2018-02-13 21:18:39     |2018-02-13 22:20:29|2018-02-14 19:46:34         |2018-02-16 18:17:02          |2018-02-26 00:00:00          |
|a4591c265e18cb1dcee52889e2d8acc3|503740e9ca751ccdda7ba28e9ab8f608|delivered   |2017-07-09 21:57:05     |2017-07-09 22:10:13|2017-07-11 14:58:04         |2017-07-26 10:57:55          |2017-08-01 00:00:00          |
|136cce7faa42fdb2cefd53fdc79a6098|ed0271e0b7da060a393796590e7b737a|invoiced    |2017-04-11 12:22:08     |2017-04-13 13:25:17|                            |                             |2017-05-09 00:00:00          |
|6514b8ad8028c9f2cc2374ded245783f|9bdf08b4b3b52b5526ff42d37d47f222|delivered   |2017-05-16 13:10:30     |2017-05-16 13:22:11|2017-05-22 10:07:46         |2017-05-26 12:55:51          |2017-06-07 00:00:00          |
|76c6e866289321a7c93b82b54852dc33|f54a9f0e6b351c431402b8461ea51999|delivered   |2017-01-23 18:29:09     |2017-01-25 02:50:47|2017-01-26 14:16:31         |2017-02-02 14:08:10          |2017-03-06 00:00:00          |
|e69bfb5eb88e0ed6a785585b27e16dbf|31ad1d1b63eb9962463f764d4e6e0c9d|delivered   |2017-07-29 11:55:02     |2017-07-29 12:05:32|2017-08-10 19:45:24         |2017-08-16 17:14:30          |2017-08-23 00:00:00          |
|e6ce16cb79ec1d90b1da9085a6118aeb|494dded5b201313c64ed7f100595b95c|delivered   |2017-05-16 19:41:10     |2017-05-16 19:50:18|2017-05-18 11:40:40         |2017-05-29 11:18:31          |2017-06-07 00:00:00          |
|34513ce0c4fab462a55830c0989c7edb|7711cf624183d843aafe81855097bc37|delivered   |2017-07-13 19:58:11     |2017-07-13 20:10:08|2017-07-14 18:43:29         |2017-07-19 14:04:48          |2017-08-08 00:00:00          |
|82566a660a982b15fb86e904c8d32918|d3e3b74c766bc6214e0c830b17ee2341|delivered   |2018-06-07 10:06:19     |2018-06-09 03:13:12|2018-06-11 13:29:00         |2018-06-19 12:05:52          |2018-07-18 00:00:00          |
|5ff96c15d0b717ac6ad1f3d77225a350|19402a48fe860416adf93348aba37740|delivered   |2018-07-25 17:44:10     |2018-07-25 17:55:14|2018-07-26 13:16:00         |2018-07-30 15:52:25          |2018-08-08 00:00:00          |
|432aaf21d85167c2c86ec9448c4e42cc|3df704f53d3f1d4818840b34ec672a9f|delivered   |2018-03-01 14:14:28     |2018-03-01 15:10:47|2018-03-02 21:09:20         |2018-03-12 23:36:26          |2018-03-21 00:00:00          |
|dcb36b511fcac050b97cd5c05de84dc3|3b6828a50ffe546942b7a473d70ac0fc|delivered   |2018-06-07 19:03:12     |2018-06-12 23:31:02|2018-06-11 14:54:00         |2018-06-21 15:34:32          |2018-07-04 00:00:00          |
|403b97836b0c04a622354cf531062e5f|738b086814c6fcc74b8cc583f8516ee3|delivered   |2018-01-02 19:00:43     |2018-01-02 19:09:04|2018-01-03 18:19:09         |2018-01-20 01:38:59          |2018-02-06 00:00:00          |
|116f0b09343b49556bbad5f35bee0cdf|3187789bec990987628d7a9beb4dd6ac|delivered   |2017-12-26 23:41:31     |2017-12-26 23:50:22|2017-12-28 18:33:05         |2018-01-08 22:36:36          |2018-01-29 00:00:00          |
|85ce859fd6dc634de8d2f1e290444043|059f7fc5719c7da6cbafe370971a8d70|delivered   |2017-11-21 00:03:41     |2017-11-21 00:14:22|2017-11-23 21:32:26         |2017-11-27 18:28:00          |2017-12-11 00:00:00          |
|83018ec114eee8641c97e08f7b4e926f|7f8c8b9c2ae27bf3300f670c3d478be8|delivered   |2017-10-26 15:54:26     |2017-10-26 16:08:14|2017-10-26 21:46:53         |2017-11-08 22:22:00          |2017-11-23 00:00:00          |
+--------------------------------+--------------------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+

-------------------------------------------
Batch: 1
-------------------------------------------
+--------------------------------+--------------------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+
|order_id                        |customer_id                     |order_status|order_purchase_timestamp|order_approved_at  |order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|
+--------------------------------+--------------------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+
|203096f03d82e0dffbc41ebc2e2bcfb7|d2b091571da224a1b36412c18bc3bbfe|delivered   |2017-09-18 14:31:30     |2017-09-19 04:04:09|2017-10-06 17:50:03         |2017-10-09 22:23:46          |2017-09-28 00:00:00          |
|f848643eec1d69395095eb3840d2051e|4fa1cd166fa598be6de80fa84eaade43|delivered   |2018-03-15 08:52:40     |2018-03-15 09:09:31|2018-03-15 19:52:48         |2018-03-19 18:08:32          |2018-03-29 00:00:00          |
|2807d0e504d6d4894d41672727bc139f|72ae281627a6102d9b3718528b420f8a|delivered   |2018-02-03 20:37:35     |2018-02-03 20:50:22|2018-02-05 22:37:28         |2018-02-08 16:13:46          |2018-02-21 00:00:00          |
|95266dbfb7e20354baba07964dac78d5|a166da34890074091a942054b36e4265|delivered   |2018-01-08 07:55:29     |2018-01-08 08:07:31|2018-01-24 23:16:37         |2018-01-26 17:32:38          |2018-02-21 00:00:00          |
|f3e7c359154d965827355f39d6b1fdac|62b423aab58096ca514ba6aa06be2f98|delivered   |2018-08-09 11:44:40     |2018-08-10 03:24:51|2018-08-10 12:29:00         |2018-08-13 18:24:27          |2018-08-17 00:00:00          |
|fbf9ac61453ac646ce8ad9783d7d0af6|3a874b4d4c4b6543206ff5d89287f0c3|delivered   |2018-02-20 23:46:53     |2018-02-22 02:30:46|2018-02-26 22:25:22         |2018-03-21 22:03:54          |2018-03-12 00:00:00          |
|acce194856392f074dbf9dada14d8d82|7e20bf5ca92da68200643bda76c504c6|delivered   |2018-06-04 00:00:13     |2018-06-05 00:35:10|2018-06-05 13:24:00         |2018-06-16 15:20:55          |2018-07-18 00:00:00          |
|dd78f560c270f1909639c11b925620ea|8b212b9525f9e74e85e37ed6df37693e|delivered   |2018-03-12 01:50:26     |2018-03-12 03:28:34|2018-03-12 21:06:37         |2018-03-21 14:41:50          |2018-03-28 00:00:00          |
|91b2a010e1e45e6ba3d133fa997597be|cce89a605105b148387c52e286ac8335|delivered   |2018-05-02 11:45:38     |2018-05-03 12:55:01|2018-05-10 16:16:00         |2018-05-16 20:56:24          |2018-05-23 00:00:00          |
|ecab90c9933c58908d3d6add7c6f5ae3|761df82feda9778854c6dafdaeb567e4|delivered   |2018-02-25 13:50:30     |2018-02-25 14:47:35|2018-02-26 22:28:50         |2018-03-27 23:29:14          |2018-04-13 00:00:00          |
|f70a0aff17df5a6cdd9a7196128bd354|456dc10730fbdba34615447ea195d643|delivered   |2017-08-10 11:58:33     |2017-08-12 02:45:24|2017-08-17 15:35:07         |2017-08-18 14:28:02          |2017-08-23 00:00:00          |
|1790eea0b567cf50911c057cf20f90f9|52142aa69d8d0e1247ab0cada0f76023|delivered   |2018-04-16 21:15:39     |2018-04-16 22:10:26|2018-04-18 13:05:09         |2018-05-05 12:28:34          |2018-05-15 00:00:00          |
|989225ba6d0ebd5873335f7e01de2ae7|816f8653d5361cbf94e58c33f2502a5c|delivered   |2017-12-12 13:56:04     |2017-12-14 13:54:13|2017-12-16 00:18:57         |2018-01-03 18:03:36          |2018-01-08 00:00:00          |
|d887b52c6516beb39e8cd44a5f8b60f7|d9ef95f98d8da3b492bb8c0447910498|delivered   |2018-02-03 12:38:58     |2018-02-03 12:50:30|2018-02-05 21:26:53         |2018-02-22 00:07:55          |2018-03-07 00:00:00          |
|b276e4f8c0fb86bd82fce576f21713e0|cf8ffeddf027932e51e4eae73b384059|delivered   |2018-07-29 23:34:51     |2018-07-29 23:45:15|2018-07-30 14:43:00         |2018-07-31 22:48:50          |2018-08-06 00:00:00          |
|8563039e855156e48fccee4d611a3196|5f16605299d698660e0606f7eae2d2f9|delivered   |2018-02-17 15:59:46     |2018-02-17 16:15:34|2018-02-20 23:03:56         |2018-03-20 00:59:25          |2018-03-20 00:00:00          |
|60550084e6b4c0cb89a87df1f3e5ebd9|f5458ddc3545711efa883dd7ae7c4497|delivered   |2018-02-21 18:15:12     |2018-02-23 02:10:52|2018-02-27 18:52:09         |2018-03-13 23:58:43          |2018-03-29 00:00:00          |
|5acce57f8d9dfd55fa48e212a641a69d|295ae9b35379e077273387ff64354b6f|delivered   |2017-07-31 21:37:10     |2017-08-02 02:56:02|2017-08-03 18:32:48         |2017-08-08 21:24:41          |2017-08-22 00:00:00          |
|434d158e96bdd6972ad6e6d73ddcfd22|2a1dfb647f32f4390e7b857c67458536|delivered   |2018-06-01 12:23:13     |2018-06-05 03:35:15|2018-06-08 11:49:00         |2018-06-18 21:32:52          |2018-07-17 00:00:00          |
|7206b86ea789983f7a273ea7fa0bc2a8|3391c4bc11a817e7973e498b0b023158|delivered   |2018-03-26 17:12:18     |2018-03-26 17:28:27|2018-03-28 17:22:53         |2018-04-05 22:11:18          |2018-04-12 00:00:00          |
+--------------------------------+--------------------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+

stream.stop()
>>> 
>>> 
>>> 
>>> # customer names #
... # read from cassandra
... customer_names = spark.read \
...     .format("org.apache.spark.sql.cassandra") \
...     .options(keyspace=keyspace, table="customer_names") \
...     .load()

>>> 
>>> customer_names.printSchema()
root
 |-- cid: string (nullable = true)
 |-- full_name: string (nullable = true)

>>> customer_names.show(truncate=False)
+--------------------------------+----------------+
|cid                             |full_name       |
+--------------------------------+----------------+
|55490de47a9c2abff0a27a5296ae9c2c|Terrance Trump  |
|ffdc2af7160aed3bad21cbc8ec68c0c5|Philip Clegg    |
|d4faa220408c20e53595d2950f361f3b|May Pollock     |
|59cf1a479d104e38587b9fe9d60febe3|Michael Dillon  |
|db925a533d94cee42707026949892c7e|Richard Beere   |
|48c83ab1910a8d95f8702dde180d658b|Martina Moore   |
|6eae55140163e2af86907a15a478f2d2|Caroline Brown  |
|4fd75fb5ef1f01c7585bf746092a1544|Andrew Booth    |
|2c5491a76ec0a56db58282ea7ae70617|David Matthews  |
|4d833833ec5cc5bb9e6c230820e4bc54|Tracey Groves   |
|fac60d7c4df3896a5af5db6c91e03797|Jane Gill       |
|aa3d8c9eec8e52e90b117efe5e97a560|Lachlan McLaren |
|686ca7499141a82f95123c370af061b0|Ibrar Majid     |
|2b47ff70d422cc3ada976dea40f48782|David Francis   |
|5072cf2f4cbec30b8ba917d5d7b6b125|Michal Buras    |
|20b5aae6a3e31111009f9a7ecc31a232|Ann Blurton     |
|bf141bf67fbe428d558bcf0e018eab60|Michael McGrane |
|e40265c6ceebf38ea830b695d9340bda|Sharon Lambe    |
|6c347ef65dd574fb9c2d654c3650fc43|Archie MacGregor|
|7e016f9ea275279784f42e0642214284|Maurice Savage  |
+--------------------------------+----------------+
only showing top 20 rows

>>> 
>>> # Static join #
... # =========== #
... orders_by_names = parsed_orders\
...     .join(customer_names, parsed_orders.customer_id == customer_names.cid, "inner")\
...     .select("order_id", customer_names.cid, customer_names.full_name)\
...     .withColumnRenamed("order_id", "oid")
>>> 
>>> 
>>> stream = console_output(orders_by_names, 10)
>>> -------------------------------------------
Batch: 0
-------------------------------------------
+--------------------------------+--------------------------------+-------------+
|oid                             |cid                             |full_name    |
+--------------------------------+--------------------------------+-------------+
|e481f51cbdc54678b7cc49136f2d6af7|9ef432eb6251297304e76186b10a928d|Kevin Maguire|
|76c6e866289321a7c93b82b54852dc33|f54a9f0e6b351c431402b8461ea51999|George Rae   |
|5ff96c15d0b717ac6ad1f3d77225a350|19402a48fe860416adf93348aba37740|Wanda Woods  |
+--------------------------------+--------------------------------+-------------+

-------------------------------------------
Batch: 1
-------------------------------------------
+---+---+---------+
|oid|cid|full_name|
+---+---+---------+
+---+---+---------+

stream.stop()
>>> # write joined df to Cassandra as Stream
... def cassandra_output(df, freq):
...     from datetime import datetime as dt
...     date = dt.now().strftime("%Y%m%d%H%M%S")
...     df.writeStream \
...         .trigger(processingTime='%s seconds' % freq) \
...         .format("org.apache.spark.sql.cassandra") \
...         .outputMode("update")\
...         .options(keyspace=keyspace, table="customer_by_order_id") \
...         .option("checkpointLocation", checkpoint_location + "/" + date)\
...         .start()
... 
>>> 
>>> stream = cassandra_output(orders_by_names, 10)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "<stdin>", line 10, in cassandra_output
  File "/spark2.4/python/pyspark/sql/streaming.py", line 1109, in start
    return self._sq(self._jwrite.start())
  File "/spark2.4/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/spark2.4/python/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/spark2.4/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o106.start.
: java.lang.UnsupportedOperationException: Data source org.apache.spark.sql.cassandra does not support streamed writing
        at org.apache.spark.sql.execution.datasources.DataSource.createSink(DataSource.scala:311)
        at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:322)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)

>>> 
>>> # write joined df to Cassandra as Stream by Batch
... def foreach_batch(df, epoch):
...     df.write\
...         .format("org.apache.spark.sql.cassandra") \
...         .mode("append")\
...         .options(keyspace=keyspace, table="customer_by_order_id") \
...         .save()
... 
>>> def cassandra_output_batch(df, freq):
...     from datetime import datetime as dt
...     date = dt.now().strftime("%Y%m%d%H%M%S")
...     return orders_by_names\
...         .writeStream\
...         .trigger(processingTime='%s seconds' % 10) \
...         .foreachBatch(foreach_batch)\
...         .option("checkpointLocation", checkpoint_location + "/" + date)\
...         .start()
... 
>>> 
>>> stream = cassandra_output_batch(orders_by_names, 10)
>>> stream.stop()
>>> # Write to Cassandra #
... '''
... OPTION              DESCRIPTION                                             VALUE TYPE      DEFAULT
... -------------------------------------------------------------------------------------------------------
... table                   The Cassandra table to connect to                       String              N/A
... keyspace            The keyspace where table is looked for                  String          N/A
... cluster                 The group of the Cluster Level Settings to inherit      String              "default"
... pushdown            Enables pushing down predicates to C* when applicable   (true,false)    true
... confirm.truncate    Confirm to truncate table when use Save.overwrite mode  (true,false)    false
... -------------------------------------------------------------------------------------------------------
... '''
'\nOPTION              DESCRIPTION                                             VALUE TYPE      DEFAULT\n-------------------------------------------------------------------------------------------------------\ntable\t            The Cassandra table to connect to\t                    String\t        N/A\nkeyspace\t        The keyspace where table is looked for\t                String\t        N/A\ncluster\t            The group of the Cluster Level Settings to inherit\t    String\t        "default"\npushdown\t        Enables pushing down predicates to C* when applicable\t(true,false)\ttrue\nconfirm.truncate\tConfirm to truncate table when use Save.overwrite mode\t(true,false)\tfalse\n-------------------------------------------------------------------------------------------------------\n'
>>> # ================== #
... names_df = spark.sql("""select '20b5aae6a3e31111009f9a7ecc31a232' as cid, 'Ann Peterson 2' as full_name""")
>>> names_df.show()
+--------------------+--------------+
|                 cid|     full_name|
+--------------------+--------------+
|20b5aae6a3e311110...|Ann Peterson 2|
+--------------------+--------------+

>>> # overwrite
... names_df.write \
...     .format("org.apache.spark.sql.cassandra") \
...     .options(keyspace=keyspace, table="customer_names") \
...     .mode("overwrite") \
...     .save()
Traceback (most recent call last):
  File "<stdin>", line 5, in <module>
  File "/spark2.4/python/pyspark/sql/readwriter.py", line 737, in save
    self._jwrite.save()
  File "/spark2.4/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/spark2.4/python/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/spark2.4/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o146.save.
: java.lang.UnsupportedOperationException: You are attempting to use overwrite mode which will truncate
this table prior to inserting data. If you would merely like
to change data already in the table use the "Append" mode.
To actually truncate please pass in true value to the option
"confirm.truncate" when saving. 
        at org.apache.spark.sql.cassandra.CassandraSourceRelation.insert(CassandraSourceRelation.scala:69)
        at org.apache.spark.sql.cassandra.DefaultSource.createRelation(DefaultSource.scala:87)
        at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)
        at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)
        at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)
        at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)
        at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)
        at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)
        at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:305)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:291)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:748)

>>> 
>>> # read all names
... all_names_df = spark.read \
...     .format("org.apache.spark.sql.cassandra") \
...     .options(keyspace=keyspace, table="customer_names") \
...     .load()
>>> 
>>> all_names_df.show()
+--------------------+----------------+
|                 cid|       full_name|
+--------------------+----------------+
|55490de47a9c2abff...|  Terrance Trump|
|ffdc2af7160aed3ba...|    Philip Clegg|
|d4faa220408c20e53...|     May Pollock|
|59cf1a479d104e385...|  Michael Dillon|
|db925a533d94cee42...|   Richard Beere|
|48c83ab1910a8d95f...|   Martina Moore|
|6eae55140163e2af8...|  Caroline Brown|
|4fd75fb5ef1f01c75...|    Andrew Booth|
|2c5491a76ec0a56db...|  David Matthews|
|4d833833ec5cc5bb9...|   Tracey Groves|
|fac60d7c4df3896a5...|       Jane Gill|
|aa3d8c9eec8e52e90...| Lachlan McLaren|
|686ca7499141a82f9...|     Ibrar Majid|
|2b47ff70d422cc3ad...|   David Francis|
|5072cf2f4cbec30b8...|    Michal Buras|
|20b5aae6a3e311110...|     Ann Blurton|
|bf141bf67fbe428d5...| Michael McGrane|
|e40265c6ceebf38ea...|    Sharon Lambe|
|6c347ef65dd574fb9...|Archie MacGregor|
|7e016f9ea27527978...|  Maurice Savage|
+--------------------+----------------+
only showing top 20 rows

>>> # filter by cid
... c_name_df = all_names_df.filter(F.col("cid") == "20b5aae6a3e31111009f9a7ecc31a232")
>>> c_name_df.show()
+--------------------+-----------+
|                 cid|  full_name|
+--------------------+-----------+
|20b5aae6a3e311110...|Ann Blurton|
+--------------------+-----------+

>>> c_name_df.count()
1
>>> # filter by non-partition key
... jane_df = all_names_df.filter(F.col("full_name") == "Ann Peterson")
>>> jane_df.show()  # only first 20
+---+---------+
|cid|full_name|
+---+---------+
+---+---------+

>>> jane_df.count()  # all records
0
>>> # PushedFilter in PhysicalPlan (explain)
... c_name_df.explain(True)
== Parsed Logical Plan ==
'Filter ('cid = 20b5aae6a3e31111009f9a7ecc31a232)
+- Relation[cid#310,full_name#311] org.apache.spark.sql.cassandra.CassandraSourceRelation@6596dcb2

== Analyzed Logical Plan ==
cid: string, full_name: string
Filter (cid#310 = 20b5aae6a3e31111009f9a7ecc31a232)
+- Relation[cid#310,full_name#311] org.apache.spark.sql.cassandra.CassandraSourceRelation@6596dcb2

== Optimized Logical Plan ==
Filter (isnotnull(cid#310) && (cid#310 = 20b5aae6a3e31111009f9a7ecc31a232))
+- Relation[cid#310,full_name#311] org.apache.spark.sql.cassandra.CassandraSourceRelation@6596dcb2

== Physical Plan ==
*(1) Filter isnotnull(cid#310)
+- *(1) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@6596dcb2 [cid#310,full_name#311] PushedFilters: [IsNotNull(cid), *EqualTo(cid,20b5aae6a3e31111009f9a7ecc31a232)], ReadSchema: struct<cid:string,full_name:string>
>>> jane_df.explain(True)
== Parsed Logical Plan ==
'Filter ('full_name = Ann Peterson)
+- Relation[cid#310,full_name#311] org.apache.spark.sql.cassandra.CassandraSourceRelation@6596dcb2

== Analyzed Logical Plan ==
cid: string, full_name: string
Filter (full_name#311 = Ann Peterson)
+- Relation[cid#310,full_name#311] org.apache.spark.sql.cassandra.CassandraSourceRelation@6596dcb2

== Optimized Logical Plan ==
Filter (isnotnull(full_name#311) && (full_name#311 = Ann Peterson))
+- Relation[cid#310,full_name#311] org.apache.spark.sql.cassandra.CassandraSourceRelation@6596dcb2

== Physical Plan ==
*(1) Filter (isnotnull(full_name#311) && (full_name#311 = Ann Peterson))
+- *(1) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@6596dcb2 [cid#310,full_name#311] PushedFilters: [IsNotNull(full_name), EqualTo(full_name,Ann Peterson)], ReadSchema: struct<cid:string,full_name:string>
>>> # in works with PushDown filter
... in_select = all_names_df.filter(F.col("cid").isin('20b5aae6a3e31111009f9a7ecc31a232', 'b89010d4a6acaa06d4ef89043869838e'))
>>> in_select.explain(True)
== Parsed Logical Plan ==
'Filter 'cid IN (20b5aae6a3e31111009f9a7ecc31a232,b89010d4a6acaa06d4ef89043869838e)
+- Relation[cid#310,full_name#311] org.apache.spark.sql.cassandra.CassandraSourceRelation@6596dcb2

== Analyzed Logical Plan ==
cid: string, full_name: string
Filter cid#310 IN (20b5aae6a3e31111009f9a7ecc31a232,b89010d4a6acaa06d4ef89043869838e)
+- Relation[cid#310,full_name#311] org.apache.spark.sql.cassandra.CassandraSourceRelation@6596dcb2

== Optimized Logical Plan ==
Filter cid#310 IN (20b5aae6a3e31111009f9a7ecc31a232,b89010d4a6acaa06d4ef89043869838e)
+- Relation[cid#310,full_name#311] org.apache.spark.sql.cassandra.CassandraSourceRelation@6596dcb2

== Physical Plan ==
*(1) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@6596dcb2 [cid#310,full_name#311] PushedFilters: [*In(cid, [20b5aae6a3e31111009f9a7ecc31a232,b89010d4a6acaa06d4ef89043869838e])], ReadSchema: struct<cid:string,full_name:string>
>>> in_select.show()
+--------------------+------------+
|                 cid|   full_name|
+--------------------+------------+
|20b5aae6a3e311110...| Ann Blurton|
|b89010d4a6acaa06d...|Stephen Wood|
+--------------------+------------+

>>> in_select.count()
21/10/02 13:09:39 WARN core.RequestHandler: Query '[2 bound values] SELECT count(*) FROM "streaming_1004"."customer_names" WHERE "cid" IN (?, ?)   ALLOW FILTERING;' generated server side warning(s): Aggregation query used on multiple partition keys (IN restriction)
2
>>> # Another READ CASSANDRA example #
... # Pushed filter #
... cass_big_df = spark.read \
...     .format("org.apache.spark.sql.cassandra") \
...     .options(table="users_many", keyspace="keyspace1") \
...     .load()
>>> 
>>> between_select = cass_big_df.filter(F.col("user_id").between(4164237664, 4164237664+10) )
>>> between_select.explain(True)
== Parsed Logical Plan ==
'Filter (('user_id >= 4164237664) && ('user_id <= 4164237674))
+- Relation[user_id#363,gender#364] org.apache.spark.sql.cassandra.CassandraSourceRelation@2589aa39

== Analyzed Logical Plan ==
user_id: string, gender: string
Filter ((cast(user_id#363 as bigint) >= 4164237664) && (cast(user_id#363 as bigint) <= 4164237674))
+- Relation[user_id#363,gender#364] org.apache.spark.sql.cassandra.CassandraSourceRelation@2589aa39

== Optimized Logical Plan ==
Filter ((isnotnull(user_id#363) && (cast(user_id#363 as bigint) >= 4164237664)) && (cast(user_id#363 as bigint) <= 4164237674))
+- Relation[user_id#363,gender#364] org.apache.spark.sql.cassandra.CassandraSourceRelation@2589aa39

== Physical Plan ==
*(1) Filter (((cast(user_id#363 as bigint) >= 4164237664) && (cast(user_id#363 as bigint) <= 4164237674)) && isnotnull(user_id#363))
+- *(1) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@2589aa39 [user_id#363,gender#364] PushedFilters: [IsNotNull(user_id)], ReadSchema: struct<user_id:string,gender:string>
>>> between_select.show()
+----------+------+                                                             
|   user_id|gender|
+----------+------+
|4164237664|     9|
|4164237669|     9|
|4164237665|     9|
|4164237673|     9|
|4164237666|     9|
|4164237667|     9|
|4164237672|     9|
|4164237674|     9|
|4164237670|     9|
|4164237671|     9|
|4164237668|     9|
+----------+------+

>>> 
>>> 
>>> in_select = cass_big_df.filter(F.col("user_id").isin(4164237664, 4164237664+1) )
>>> in_select.explain(True)
== Parsed Logical Plan ==
'Filter 'user_id IN (4164237664,4164237665)
+- Relation[user_id#363,gender#364] org.apache.spark.sql.cassandra.CassandraSourceRelation@2589aa39

== Analyzed Logical Plan ==
user_id: string, gender: string
Filter cast(user_id#363 as string) IN (cast(4164237664 as string),cast(4164237665 as string))
+- Relation[user_id#363,gender#364] org.apache.spark.sql.cassandra.CassandraSourceRelation@2589aa39

== Optimized Logical Plan ==
Filter user_id#363 IN (4164237664,4164237665)
+- Relation[user_id#363,gender#364] org.apache.spark.sql.cassandra.CassandraSourceRelation@2589aa39

== Physical Plan ==
*(1) Scan org.apache.spark.sql.cassandra.CassandraSourceRelation@2589aa39 [user_id#363,gender#364] PushedFilters: [*In(user_id, [4164237664,4164237665])], ReadSchema: struct<user_id:string,gender:string>
>>> in_select.show()
+----------+------+
|   user_id|gender|
+----------+------+
|4164237664|     9|
|4164237665|     9|
+----------+------+

>>> 
[student910_14@bigdataanalytics2-worker-shdpt-v31-1-0 ~]$ logout
Connection to 10.0.0.19 closed.
[student910_14@bigdataanalytics2-head-shdpt-v31-1-0 ~]$ logout
Connection to 37.139.32.56 closed.
    ~                                                                                                                                          ✔  1h 32m 15s   base  
